{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2023-08-01T19:09:29.499052Z","iopub.execute_input":"2023-08-01T19:09:29.499462Z","iopub.status.idle":"2023-08-01T19:09:29.507968Z","shell.execute_reply.started":"2023-08-01T19:09:29.499424Z","shell.execute_reply":"2023-08-01T19:09:29.506749Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Plan \n1. Data analysis\n2. EDA and hypothesys building\n3. Preprocessing of the data before training\n4. Model selection\n5. Data separation for train and validation\n6. Building and training th model\n7. Model validation \n8. Reevaluation of steps to have the best score\n9. Submission and praying the score is good\n","metadata":{}},{"cell_type":"code","source":"# Imports\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport numpy as np\nfrom scipy.stats import norm\nfrom sklearn.preprocessing import StandardScaler\nfrom scipy import stats\nimport warnings\nwarnings.filterwarnings('ignore')\n%matplotlib inline\n\n# Loading data\ndf_train = pd.read_csv('../input/house-prices-advanced-regression-techniques/train.csv')\ndf_test = pd.read_csv('../input/house-prices-advanced-regression-techniques/test.csv')\nids = df_test['Id'].values","metadata":{"execution":{"iopub.status.busy":"2023-08-01T19:09:29.51054Z","iopub.execute_input":"2023-08-01T19:09:29.510985Z","iopub.status.idle":"2023-08-01T19:09:29.56973Z","shell.execute_reply.started":"2023-08-01T19:09:29.510944Z","shell.execute_reply":"2023-08-01T19:09:29.568629Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Data analysis","metadata":{}},{"cell_type":"code","source":"df_train.describe()","metadata":{"execution":{"iopub.status.busy":"2023-08-01T19:09:29.636878Z","iopub.execute_input":"2023-08-01T19:09:29.637981Z","iopub.status.idle":"2023-08-01T19:09:29.752523Z","shell.execute_reply.started":"2023-08-01T19:09:29.637939Z","shell.execute_reply":"2023-08-01T19:09:29.751672Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"First, lets learn more about Sales prices","metadata":{}},{"cell_type":"code","source":"df_train['SalePrice'].describe()","metadata":{"execution":{"iopub.status.busy":"2023-08-01T19:09:29.754214Z","iopub.execute_input":"2023-08-01T19:09:29.754536Z","iopub.status.idle":"2023-08-01T19:09:29.768627Z","shell.execute_reply.started":"2023-08-01T19:09:29.75451Z","shell.execute_reply":"2023-08-01T19:09:29.767692Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"sns.distplot(df_train['SalePrice'])","metadata":{"execution":{"iopub.status.busy":"2023-08-01T19:09:29.7702Z","iopub.execute_input":"2023-08-01T19:09:29.770562Z","iopub.status.idle":"2023-08-01T19:09:30.180503Z","shell.execute_reply.started":"2023-08-01T19:09:29.770523Z","shell.execute_reply":"2023-08-01T19:09:30.179398Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"The data is clearly skewed, but no visible or unnatural oultiers are present. There are also no values that are less than 0 and all values make sence for the house prices.  Now, Ill create X and y train df.","metadata":{}},{"cell_type":"code","source":"y_train = df_train.SalePrice.values\nx_train = df_train.drop('SalePrice', 1)","metadata":{"execution":{"iopub.status.busy":"2023-08-01T19:09:30.181982Z","iopub.execute_input":"2023-08-01T19:09:30.182303Z","iopub.status.idle":"2023-08-01T19:09:30.18934Z","shell.execute_reply.started":"2023-08-01T19:09:30.182276Z","shell.execute_reply":"2023-08-01T19:09:30.188159Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"After reading some of the notebooks, I've identified several columns that seemed to be the most important to note:\n1. OverallQual - I don't know how this value was calulated, but the correlation is clear. A very useful column.\n2. YearBuilt - categorical feature. During the data analysis it was decided not to use this cilumn due to no clear trend being present.\n3. TotalBsmtSF.\n4. GrLivArea.\n5. Neighborhood. Not all notebooks talk about it, but knowing the market, I think this value might be important so I've decided to test it out too. ","metadata":{}},{"cell_type":"markdown","source":"### Visualizing categorical features","metadata":{}},{"cell_type":"code","source":"data = pd.concat([df_train['SalePrice'], df_train['OverallQual']], axis=1)\nplt.figure(figsize=(8, 6))\nsns.boxplot(x='OverallQual', y=\"SalePrice\", data=data)","metadata":{"execution":{"iopub.status.busy":"2023-08-01T19:09:30.192209Z","iopub.execute_input":"2023-08-01T19:09:30.192543Z","iopub.status.idle":"2023-08-01T19:09:30.928125Z","shell.execute_reply.started":"2023-08-01T19:09:30.192516Z","shell.execute_reply":"2023-08-01T19:09:30.927041Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"There is a clear trend. Now visualizing numerical data","metadata":{}},{"cell_type":"code","source":"data = pd.concat([df_train['SalePrice'], df_train['GrLivArea']], axis=1)\ndata.plot.scatter(x='GrLivArea', y='SalePrice')","metadata":{"execution":{"iopub.status.busy":"2023-08-01T19:09:30.929591Z","iopub.execute_input":"2023-08-01T19:09:30.929908Z","iopub.status.idle":"2023-08-01T19:09:31.209181Z","shell.execute_reply.started":"2023-08-01T19:09:30.929881Z","shell.execute_reply":"2023-08-01T19:09:31.208052Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"data = pd.concat([df_train['SalePrice'], df_train['TotalBsmtSF']], axis=1)\ndata.plot.scatter(x='TotalBsmtSF', y='SalePrice')","metadata":{"execution":{"iopub.status.busy":"2023-08-01T19:09:31.2104Z","iopub.execute_input":"2023-08-01T19:09:31.211225Z","iopub.status.idle":"2023-08-01T19:09:31.507123Z","shell.execute_reply.started":"2023-08-01T19:09:31.211193Z","shell.execute_reply":"2023-08-01T19:09:31.505966Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Linear trend visible for numerical data. now, I want to visualize the Neighborhood to see if there is any visible trend","metadata":{}},{"cell_type":"code","source":"data = pd.concat([df_train['SalePrice'], df_train['Neighborhood']], axis=1)\nplt.figure(figsize=(20, 6))\nsns.boxplot(x='Neighborhood', y=\"SalePrice\", data=data)","metadata":{"execution":{"iopub.status.busy":"2023-08-01T19:09:31.508596Z","iopub.execute_input":"2023-08-01T19:09:31.509488Z","iopub.status.idle":"2023-08-01T19:09:32.175908Z","shell.execute_reply.started":"2023-08-01T19:09:31.509454Z","shell.execute_reply":"2023-08-01T19:09:32.174813Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"No trend visible, but there are clearly more pricier neighborhoods than others. NridgHt and StoneBr both have a very variale price and some other the biggest outliers of the set. ","metadata":{}},{"cell_type":"markdown","source":"Chechking the heatmap","metadata":{}},{"cell_type":"code","source":"#correlation matrix\ncorrmat = df_train.corr()\nplt.figure(figsize=(12, 12))\nsns.heatmap(corrmat, vmax=.8, square=True)","metadata":{"execution":{"iopub.status.busy":"2023-08-01T19:09:32.177542Z","iopub.execute_input":"2023-08-01T19:09:32.177942Z","iopub.status.idle":"2023-08-01T19:09:33.138213Z","shell.execute_reply.started":"2023-08-01T19:09:32.177912Z","shell.execute_reply":"2023-08-01T19:09:33.137115Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Overall Qual has the highest correlations with the Sale Price which proves that it is a very important factor for Sale Price predction. We can also see that LotArea surpisignly doesn't affect the price as much. ","metadata":{}},{"cell_type":"markdown","source":"## Handling missing values","metadata":{}},{"cell_type":"code","source":"total = df_train.isnull().sum().sort_values(ascending=False)\npercent = (df_train.isnull().sum()/df_train.isnull().count()).sort_values(ascending=False)\nmissing_data = pd.concat([total, percent], axis=1, keys=['Total', 'Percent'])\nmissing_data.head(20)","metadata":{"execution":{"iopub.status.busy":"2023-08-01T19:09:33.139811Z","iopub.execute_input":"2023-08-01T19:09:33.14016Z","iopub.status.idle":"2023-08-01T19:09:33.187532Z","shell.execute_reply.started":"2023-08-01T19:09:33.14013Z","shell.execute_reply":"2023-08-01T19:09:33.186443Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"First 6 candidates are not affecting the Sale Price too much, but they have too many missing values. We can simply drop them. To fill the rest of the values, I would replace the na with the most common value for the category. ","metadata":{}},{"cell_type":"code","source":"x_train = x_train.drop((missing_data[missing_data['Total'] > 81]).index,1)\nx_train = x_train.apply(lambda x:x.fillna(x.value_counts().index[0]))\nx_train.isnull().sum().max()","metadata":{"execution":{"iopub.status.busy":"2023-08-01T19:09:33.189239Z","iopub.execute_input":"2023-08-01T19:09:33.189602Z","iopub.status.idle":"2023-08-01T19:09:33.263702Z","shell.execute_reply.started":"2023-08-01T19:09:33.189573Z","shell.execute_reply":"2023-08-01T19:09:33.26263Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"x_train.shape","metadata":{"execution":{"iopub.status.busy":"2023-08-01T19:09:33.265139Z","iopub.execute_input":"2023-08-01T19:09:33.265574Z","iopub.status.idle":"2023-08-01T19:09:33.272347Z","shell.execute_reply.started":"2023-08-01T19:09:33.265542Z","shell.execute_reply":"2023-08-01T19:09:33.271272Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Now, let's deal with the messing values in test data. ","metadata":{}},{"cell_type":"code","source":"df_test.info()","metadata":{"execution":{"iopub.status.busy":"2023-08-01T19:09:33.274204Z","iopub.execute_input":"2023-08-01T19:09:33.274767Z","iopub.status.idle":"2023-08-01T19:09:33.30339Z","shell.execute_reply.started":"2023-08-01T19:09:33.274726Z","shell.execute_reply":"2023-08-01T19:09:33.302278Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df_test = df_test.drop((missing_data[missing_data['Total'] > 81]).index,1)\ndf_test = df_test.apply(lambda x:x.fillna(x.value_counts().index[0]))","metadata":{"execution":{"iopub.status.busy":"2023-08-01T19:09:33.308704Z","iopub.execute_input":"2023-08-01T19:09:33.309051Z","iopub.status.idle":"2023-08-01T19:09:33.373673Z","shell.execute_reply.started":"2023-08-01T19:09:33.309021Z","shell.execute_reply":"2023-08-01T19:09:33.372391Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Data preprocessing\n\nWe can now drop the Id columns, as they are unique and not informative for the model ","metadata":{}},{"cell_type":"code","source":"x_train.drop(\"Id\", axis = 1, inplace = True)\ndf_test.drop(\"Id\", axis = 1, inplace = True)\nx_train.shape","metadata":{"execution":{"iopub.status.busy":"2023-08-01T19:09:33.37561Z","iopub.execute_input":"2023-08-01T19:09:33.376049Z","iopub.status.idle":"2023-08-01T19:09:33.388104Z","shell.execute_reply.started":"2023-08-01T19:09:33.376011Z","shell.execute_reply":"2023-08-01T19:09:33.386735Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Encoding categorical values","metadata":{}},{"cell_type":"code","source":"\nfrom sklearn.preprocessing import LabelEncoder\ncols = x_train.select_dtypes(include='object').columns\n\nfor c in cols:\n    lbl = LabelEncoder() \n    lbl.fit(list(x_train[c].values)) \n    x_train[c] = lbl.transform(list(x_train[c].values))\n    df_test[c] = lbl.transform(list(df_test[c].values))\n\nprint('Shape all_data: {}'.format(x_train.shape))","metadata":{"execution":{"iopub.status.busy":"2023-08-01T19:09:33.38955Z","iopub.execute_input":"2023-08-01T19:09:33.390006Z","iopub.status.idle":"2023-08-01T19:09:33.531061Z","shell.execute_reply.started":"2023-08-01T19:09:33.389965Z","shell.execute_reply":"2023-08-01T19:09:33.529807Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Removing some outliers","metadata":{}},{"cell_type":"code","source":"indexes = x_train[(df_train['GrLivArea']>4000) & (df_train['SalePrice']<300000)].index \n\nx_train = x_train.drop(indexes)\ny_train = np.delete(y_train, indexes)\ny_train.shape","metadata":{"execution":{"iopub.status.busy":"2023-08-01T19:09:33.532284Z","iopub.execute_input":"2023-08-01T19:09:33.532648Z","iopub.status.idle":"2023-08-01T19:09:33.548027Z","shell.execute_reply.started":"2023-08-01T19:09:33.532618Z","shell.execute_reply":"2023-08-01T19:09:33.546713Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Creating a model","metadata":{}},{"cell_type":"code","source":"from sklearn.linear_model import ElasticNet, Lasso,  BayesianRidge, LassoLarsIC\nfrom sklearn.ensemble import RandomForestRegressor,  GradientBoostingRegressor\nfrom sklearn.kernel_ridge import KernelRidge\nfrom sklearn.pipeline import make_pipeline\nfrom sklearn.preprocessing import RobustScaler\nfrom sklearn.base import BaseEstimator, TransformerMixin, RegressorMixin, clone\nfrom sklearn.model_selection import KFold, cross_val_score, train_test_split\nfrom sklearn.metrics import mean_squared_error\nimport xgboost as xgb","metadata":{"execution":{"iopub.status.busy":"2023-08-01T19:09:33.552426Z","iopub.execute_input":"2023-08-01T19:09:33.552795Z","iopub.status.idle":"2023-08-01T19:09:33.558882Z","shell.execute_reply.started":"2023-08-01T19:09:33.552767Z","shell.execute_reply":"2023-08-01T19:09:33.557587Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"I start with XGB boost","metadata":{}},{"cell_type":"code","source":"model_xgb = xgb.XGBRegressor(n_estimators=2200)\nn_folds = 5\n\ndef rmsle(model):\n    kf = KFold(n_folds, shuffle=True, random_state=42).get_n_splits(train.values)\n    rmse = np.sqrt(-cross_val_score(model, train.values, y_train, scoring=\"neg_mean_squared_error\", cv = kf))\n    return(rmse)\n\ndef rmsle(y, y_pred):\n    return np.sqrt(mean_squared_error(y, y_pred))","metadata":{"execution":{"iopub.status.busy":"2023-08-01T19:09:33.56044Z","iopub.execute_input":"2023-08-01T19:09:33.561135Z","iopub.status.idle":"2023-08-01T19:09:33.578922Z","shell.execute_reply.started":"2023-08-01T19:09:33.561103Z","shell.execute_reply":"2023-08-01T19:09:33.577873Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Fit model and generate predictions","metadata":{}},{"cell_type":"code","source":"model_xgb.fit(x_train, y_train)\nxgb_train_pred = model_xgb.predict(x_train)\nxgb_pred = model_xgb.predict(df_test)\nprint(rmsle(y_train, xgb_train_pred))","metadata":{"execution":{"iopub.status.busy":"2023-08-01T19:09:33.580557Z","iopub.execute_input":"2023-08-01T19:09:33.580885Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"This can still be improved. let's try GridSearch","metadata":{}},{"cell_type":"code","source":"from sklearn.model_selection import GridSearchCV\n\nparams = {'min_child_weight':[4,5], 'gamma':[i/10.0 for i in range(3,6)],  'subsample':[i/10.0 for i in range(6,11)],\n'colsample_bytree':[i/10.0 for i in range(4,11)], 'max_depth': [2,3,4], 'reg_lambda':[i/10.0 for i in range(7,9)], 'reg_alpha':[i/10.0 for i in range(4,7)]}\n\nmodel = xgb.XGBRegressor(nthread=-1) \n\n#grid = GridSearchCV(model, params)\n#grid.fit(x_train, y_train)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"This takes too long, and according to other notebooks, the score is not improved. Next, I will try stacking the models","metadata":{}},{"cell_type":"code","source":"from sklearn.base import BaseEstimator, TransformerMixin, RegressorMixin\nfrom sklearn.kernel_ridge import KernelRidge\nfrom sklearn.ensemble import GradientBoostingRegressor\nfrom sklearn.pipeline import make_pipeline\nfrom sklearn.preprocessing import RobustScaler\nfrom sklearn.linear_model import ElasticNet, Lasso\nimport lightgbm as lgb\n\nclass StackingAveragedModels(BaseEstimator, RegressorMixin, TransformerMixin):\n    def __init__(self, base_models, meta_model, n_folds=5):\n        self.base_models = base_models\n        self.meta_model = meta_model\n        self.n_folds = n_folds\n   \n    # We again fit the data on clones of the original models\n    def fit(self, X, y):\n        self.base_models_ = [list() for x in self.base_models]\n        self.meta_model_ = clone(self.meta_model)\n        kfold = KFold(n_splits=self.n_folds, shuffle=True, random_state=156)\n        \n        # Train cloned base models then create out-of-fold predictions\n        # that are needed to train the cloned meta-model\n        out_of_fold_predictions = np.zeros((X.shape[0], len(self.base_models)))\n        for i, model in enumerate(self.base_models):\n            for train_index, holdout_index in kfold.split(X, y):\n                instance = clone(model)\n                self.base_models_[i].append(instance)\n                instance.fit(X[train_index], y[train_index])\n                y_pred = instance.predict(X[holdout_index])\n                out_of_fold_predictions[holdout_index, i] = y_pred\n                \n        # Now train the cloned  meta-model using the out-of-fold predictions as new feature\n        self.meta_model_.fit(out_of_fold_predictions, y)\n        return self\n   \n    #Do the predictions of all base models on the test data and use the averaged predictions as \n    #meta-features for the final prediction which is done by the meta-model\n    def predict(self, X):\n        meta_features = np.column_stack([\n            np.column_stack([model.predict(X) for model in base_models]).mean(axis=1)\n            for base_models in self.base_models_ ])\n        return self.meta_model_.predict(meta_features)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"KRR = KernelRidge(alpha=0.6, kernel='polynomial', degree=2, coef0=2.5)\n\nGBoost = GradientBoostingRegressor(n_estimators=3000, random_state =42)\n\nENet = make_pipeline(RobustScaler(), ElasticNet(alpha=0.0005,random_state=42))\nlasso = make_pipeline(RobustScaler(), Lasso(alpha =0.0005, random_state=42))\n\nstacked_averaged_models = StackingAveragedModels(base_models = (ENet, GBoost, KRR),\n                                                 meta_model = lasso)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"model_xgb = xgb.XGBRegressor(n_estimators=2200, nthread = -1)\nmodel_xgb.fit(x_train, y_train)\n\nmodel_lgb = lgb.LGBMRegressor(objective='regression',n_estimators=720)\nmodel_lgb.fit(x_train, y_train)\n\nstacked_averaged_models.fit(x_train.values, y_train)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"lgb_pred = model_lgb.predict(df_test)\nxgb_pred = model_xgb.predict(df_test)\nstacked_pred = stacked_averaged_models.predict(df_test.values)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"ensemble = stacked_pred*0.70 + xgb_pred*0.15 + lgb_pred*0.15","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"sub = pd.DataFrame()\nsub['Id'] = ids\nsub['SalePrice'] = xgb_pred\nsub.to_csv('submission.csv',index=False)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"There is still a lot of room to go and the model can be further fine tuned and improved","metadata":{}},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}